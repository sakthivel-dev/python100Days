{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation metrics for classification models\n",
    "\n",
    "Evaluation Metrics for Classification Models\n",
    "\n",
    "Classification models are used to predict categorical outcomes, such as \"yes\" or \"no,\" \"spam\" or \"not spam,\" or class labels like \"cat,\" \"dog,\" or \"horse.\" \n",
    "\n",
    "To assess the performance of these models, various evaluation metrics are employed. \n",
    "\n",
    "The choice of metric depends on the nature of the problem (binary or multiclass), the class distribution, and specific objectives. \n",
    "\n",
    "Here are some commonly used evaluation metrics for classification models:\n",
    "\n",
    "1. Accuracy:\n",
    "\n",
    "Formula: (TP + TN) / (TP + TN + FP + FN)\n",
    "\n",
    "Description: Measures the proportion of correctly classified instances out of the total.\n",
    "\n",
    "Use Cases: Suitable for balanced datasets. \n",
    "\n",
    "However, it may not be the best metric when dealing with imbalanced data.\n",
    "\n",
    "2. Precision:\n",
    "\n",
    "Formula: TP / (TP + FP)\n",
    "\n",
    "Description: Measures the accuracy of positive predictions. \n",
    "\n",
    "    It answers, \"Of the instances predicted as positive, how many were actually positive?\"\n",
    "\n",
    "Use Cases: Useful when the cost of false positives is high \n",
    "\n",
    "(e.g., medical diagnosis, fraud detection).\n",
    "\n",
    "3. Recall (Sensitivity or True Positive Rate):\n",
    "\n",
    "Formula: TP / (TP + FN)\n",
    "\n",
    "Description: Measures the ability of the model to correctly identify positive instances. \n",
    "\n",
    "It answers, \"Of all actual positive instances, how many were correctly predicted?\"\n",
    "\n",
    "Use Cases: Important when missing a positive instance has high consequences \n",
    "\n",
    "(e.g., disease detection).\n",
    "\n",
    "4. F1-Score:\n",
    "\n",
    "Formula: 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "Description: Harmonic mean of precision and recall. Balances precision and recall.\n",
    "\n",
    "Use Cases: Suitable when seeking a balance between false positives and false negatives.\n",
    "\n",
    "5. Specificity (True Negative Rate):\n",
    "\n",
    "Formula: TN / (TN + FP)\n",
    "\n",
    "Description: Measures the ability to correctly identify negative instances. \n",
    "\n",
    "    It answers, \"Of all actual negative instances, how many were correctly predicted as negative?\"\n",
    "\n",
    "Use Cases: Relevant in scenarios where correctly identifying negatives is crucial \n",
    "\n",
    "(e.g., manufacturing quality control)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. ROC Curve (Receiver Operating Characteristic):\n",
    "\n",
    "Description: Graphical representation of the model's performance at various thresholds. \n",
    "\n",
    "    It plots the true positive rate (recall) against the false positive rate (1-specificity) at different decision thresholds.\n",
    "\n",
    "Use Cases: Helps choose an appropriate threshold based on the trade-off between false positives and true positives.\n",
    "\n",
    "7. AUC-ROC (Area Under the ROC Curve):\n",
    "\n",
    "Description: Measures the overall performance of a binary classification model. \n",
    "\n",
    "    It quantifies the model's ability to discriminate between positive and negative instances across all possible threshold values.\n",
    "\n",
    "Use Cases: A high AUC-ROC score indicates good model performance.\n",
    "\n",
    "8. Precision-Recall Curve:\n",
    "\n",
    "Description: Graphical representation of precision and recall at different decision thresholds. \n",
    "\n",
    "It helps in choosing an appropriate threshold based on the trade-off between precision and recall.\n",
    "\n",
    "9. F-beta Score:\n",
    "\n",
    "Formula: (1 + β^2) * (Precision * Recall) / (β^2 * Precision + Recall)\n",
    "\n",
    "Description: A generalized version of the F1-score where β controls the balance between precision and recall. \n",
    "\n",
    "F1 is a special case when β = 1.\n",
    "\n",
    "Use Cases: Allows you to emphasize either precision (β < 1) or recall (β > 1) based on your objectives.\n",
    "\n",
    "10. Confusion Matrix:\n",
    "\n",
    "- Description: A table that summarizes the true positives, true negatives, false positives, and false negatives, providing a detailed breakdown of the model's performance.\n",
    "\n",
    "The choice of evaluation metric(s) depends on the problem at hand, the dataset, and the specific goals of your machine learning project. \n",
    "\n",
    "It's often advisable to consider multiple metrics to gain a comprehensive understanding of a classification model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
