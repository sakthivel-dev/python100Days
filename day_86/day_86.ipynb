{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation in Neural Networks\n",
    "\n",
    "Forward propagation is the process by which input data is processed through a neural network to generate predictions or classifications. \n",
    "\n",
    "It involves passing the data through the network layer by layer, with each layer performing a series of calculations. \n",
    "\n",
    "Here's a step-by-step explanation of forward propagation:\n",
    "\n",
    "1. Input Layer:\n",
    "\n",
    "* The forward propagation process begins at the input layer.\n",
    "\n",
    "* Each neuron in the input layer corresponds to a feature or element in the input data.\n",
    "\n",
    "* The input data is fed into the neurons in the input layer.\n",
    "\n",
    "2. Weighted Sum and Bias:\n",
    "\n",
    "For each neuron in the hidden and output layers, there are associated weights and a bias.\n",
    "\n",
    "The weighted sum of the input values for each neuron is calculated as follows:\n",
    "\n",
    "<!-- weighted_sum = (input_1 * weight_1) + (input_2 * weight_2) + ... + (input_n * weight_n)-->\n",
    "\n",
    " Here, input_i is the input from neuron i in the previous layer, and weight_i is the weight associated with the connection between the two neurons.\n",
    "\n",
    " The bias term is added to the weighted sum:\n",
    "\n",
    " <!-- weighted_sum_with_bias = weighted_sum + bias -->\n",
    "\n",
    " 3. Activation Function:\n",
    "\n",
    "The weighted sum with bias is passed through an activation function, which introduces non-linearity into the network.\n",
    "\n",
    "Common activation functions include ReLU, sigmoid, tanh, and others.\n",
    "\n",
    "The choice of activation function depends on the problem and network architecture.\n",
    "\n",
    "4. Output of Neurons:\n",
    "\n",
    "The output of each neuron in the current layer is the result of applying the activation function to the weighted sum with bias.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Passing to the Next Layer:\n",
    "\n",
    "The outputs of the neurons in the current layer become the inputs to the neurons in the next layer.\n",
    "\n",
    "This process repeats for each layer in the network, from the input layer to the output layer.\n",
    "\n",
    "6. Final Output:\n",
    "\n",
    "After the data has passed through all the hidden layers, it reaches the output layer.\n",
    "\n",
    "The output layer generates the final predictions or classifications based on the processed information.\n",
    "\n",
    "7. Prediction or Classification:\n",
    "\n",
    "The output of the neural network's output layer represents the network's prediction or classification.\n",
    "\n",
    "For example, in a binary classification problem, a single output neuron may produce a probability value \n",
    "\n",
    "    that indicates the likelihood of belonging to one class, with 0.5 as the decision threshold.\n",
    "\n",
    "8. Loss Calculation:\n",
    "\n",
    "After forward propagation, the network's prediction is compared to the actual target values (ground truth).\n",
    "\n",
    "A loss or error function is used to quantify the difference between the predicted and actual values. \n",
    "\n",
    "Common loss functions include mean squared error (MSE) for regression and cross-entropy loss for classification.\n",
    "\n",
    "Forward propagation is just one part of the neural network training process. \n",
    "\n",
    "After forward propagation, the network typically undergoes backward propagation (backpropagation) to adjust its weights and \n",
    "\n",
    "biases based on the calculated loss. This adjustment helps the network improve its predictions through gradient descent optimization techniques.\n",
    "\n",
    "The entire forward and backward propagation process is iteratively repeated during training until the network's performance reaches a satisfactory level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
