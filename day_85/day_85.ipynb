{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation functions: Sigmoid, ReLU, and more\n",
    "\n",
    "Activation functions are crucial components in artificial neural networks and deep learning models. \n",
    "\n",
    "They introduce non-linearity to the network, allowing it to learn complex relationships in the data. \n",
    "\n",
    "Here are some commonly used activation functions:\n",
    "\n",
    "1. Sigmoid Activation Function:\n",
    "\n",
    "* Range: (0, 1)\n",
    "\n",
    "* Description: Sigmoid squashes the input values to the range (0, 1), making it suitable for binary classification problems. \n",
    "\n",
    "    It's differentiable, which is important for gradient-based optimization during training. \n",
    "    \n",
    "    However, it has a vanishing gradient problem, which can slow down training in deep networks.\n",
    "\n",
    "2. Hyperbolic Tangent (Tanh) Activation Function:\n",
    "\n",
    "* Range: (-1, 1)\n",
    "\n",
    "* Description: Tanh is similar to the sigmoid but squashes input values to the range (-1, 1). \n",
    "\n",
    "    It has the advantage of being zero-centered, which can speed up convergence in some cases compared to sigmoid. \n",
    "    \n",
    "    However, it also suffers from the vanishing gradient problem.\n",
    "\n",
    "3. Rectified Linear Unit (ReLU) Activation Function:\n",
    "\n",
    "* Range: [0, ∞)\n",
    "\n",
    "* Description: ReLU is one of the most popular activation functions. \n",
    "\n",
    "    It's computationally efficient and encourages sparse representations in neural networks. \n",
    "\n",
    "    However, it's not differentiable at zero (subgradients are used in practice), \n",
    "    \n",
    "    and it can suffer from a problem known as the \"dying ReLU\" problem, where some neurons may become \n",
    "    \n",
    "    inactive during training and never recover.\n",
    "\n",
    "4. Leaky Rectified Linear Unit (Leaky ReLU) Activation Function:\n",
    "\n",
    "* Range: (-∞, ∞)\n",
    "\n",
    "* Description: Leaky ReLU addresses the \"dying ReLU\" problem by allowing a small, \n",
    "\n",
    "    non-zero gradient for negative input values (controlled by the hyperparameter α). \n",
    "    \n",
    "    This helps prevent neurons from becoming inactive during training.\n",
    "\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Parametric Rectified Linear Unit (PReLU) Activation Function:\n",
    "\n",
    "* Range: (-∞, ∞)\n",
    "\n",
    "* Description: PReLU is similar to Leaky ReLU but allows the slope (α) to be learned during training, \n",
    "\n",
    "    rather than being a fixed hyperparameter. This gives the model more flexibility.\n",
    "\n",
    "6. Exponential Linear Unit (ELU) Activation Function:\n",
    "\n",
    "* Range: (-α, ∞)\n",
    "\n",
    "* Description: ELU is a variant of ReLU that also addresses the \"dying ReLU\" problem. \n",
    "\n",
    "    It has smooth gradients for both positive and negative inputs and can help improve training speed.\n",
    "\n",
    "7. Swish Activation Function:\n",
    "\n",
    "* Range: (-∞, ∞)\n",
    "\n",
    "* Description: Swish is a newer activation function that is self-gated and has properties of both sigmoid (smoothness) \n",
    "\n",
    "    and ReLU (activation function). It has shown promising results in some deep learning applications.\n",
    "\n",
    "8. Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM):\n",
    "\n",
    "These are specialized activation functions used in recurrent neural networks (RNNs) for sequential data. \n",
    "\n",
    "GRU and LSTM cells include various gating mechanisms to control information flow and combat vanishing gradient problems.\n",
    "\n",
    "The choice of activation function depends on the specific problem, network architecture, and empirical testing. \n",
    "\n",
    "ReLU and its variants are often preferred due to their effectiveness and computational efficiency. \n",
    "\n",
    "However, it's essential to be aware of their characteristics and potential issues, such as the \"dying ReLU\" problem or \n",
    "\n",
    "vanishing gradients, when designing and training deep neural networks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
